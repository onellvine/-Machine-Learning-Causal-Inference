{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Lalonde NSW Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A. Load the Lalonde experimental dataset with the lalonde_data method from the module causalinference.utils. Using CausalModel from the module causalinference, provide summary statistics for the outcome variable and the covariates. Which covariate has the largest normalized difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalinference.utils import lalonde_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset\n",
    "data = lalonde_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalinference import CausalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[0]  # observed outcomes\n",
    "D = data[1]  # treatment status indicators\n",
    "X = data[2]  # the matrix of covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CausalModel object\n",
    "causal = CausalModel(Y, D, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics\n",
      "\n",
      "                       Controls (N_c=260)         Treated (N_t=185)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        4.555        5.484        6.349        7.867        1.794\n",
      "\n",
      "                       Controls (N_c=260)         Treated (N_t=185)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        0.827        0.379        0.843        0.365        0.044\n",
      "             X1        0.108        0.311        0.059        0.237       -0.175\n",
      "             X2       25.054        7.058       25.816        7.155        0.107\n",
      "             X3        0.154        0.361        0.189        0.393        0.094\n",
      "             X4        0.835        0.372        0.708        0.456       -0.304\n",
      "             X5       10.088        1.614       10.346        2.011        0.141\n",
      "             X6        2.107        5.688        2.096        4.887       -0.002\n",
      "             X7        0.750        0.434        0.708        0.456       -0.094\n",
      "             X8        1.267        3.103        1.532        3.219        0.084\n",
      "             X9        0.685        0.466        0.600        0.491       -0.177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary statistics for the outcome variable and the covariates\n",
    "summary_stats = causal.summary_stats\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['N', 'K', 'N_c', 'N_t', 'Y_c_mean', 'Y_t_mean', 'Y_c_sd', 'Y_t_sd', 'rdiff', 'X_c_mean', 'X_t_mean', 'X_c_sd', 'X_t_sd', 'ndiff'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the summary_stats is a python dictionary, its keys are \n",
    "summary_stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy to locate the index of an element in an ndarray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The covariate with the largest normalized difference is X5\n"
     ]
    }
   ],
   "source": [
    "# the ndiff represents the normalized difference\n",
    "normalized_diff = summary_stats['ndiff']\n",
    "\n",
    "# the covariate with the largest normalized difference\n",
    "max_ndiff = np.where(normalized_diff == [max(normalized_diff)])[0][0]\n",
    "\n",
    "print(f\"The covariate with the largest normalized difference is X{max_ndiff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### B. Estimate the propensity score using the selection algorithm est_propensity_s. In selecting the basic covariates set, specify E74, U74, E75, and U75. What are the additional linear terms and second-order terms that were selected by the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the propensity score\n",
    "# using 6,7,8,9 as the column numbers representing E74, U74, E75, U75\n",
    "causal.est_propensity_s(lin_B=[6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept     -3.480      4.471     -0.778      0.436    -12.243      5.283\n",
      "            X6      0.034      0.051      0.667      0.505     -0.066      0.133\n",
      "            X7     -0.236      0.386     -0.611      0.541     -0.992      0.521\n",
      "            X8      0.058      0.051      1.144      0.253     -0.041      0.158\n",
      "            X9     -3.477      1.652     -2.104      0.035     -6.716     -0.238\n",
      "            X4      7.329      4.255      1.723      0.085     -1.010     15.668\n",
      "            X1     -0.653      0.385     -1.696      0.090     -1.409      0.102\n",
      "            X5      0.290      0.370      0.783      0.433     -0.435      1.015\n",
      "         X4*X5     -0.668      0.349     -1.915      0.056     -1.352      0.016\n",
      "         X6*X4     -0.130      0.057     -2.286      0.022     -0.241     -0.018\n",
      "         X9*X5      0.304      0.156      1.950      0.051     -0.002      0.609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# propensity score\n",
    "propensity = causal.propensity\n",
    "print(propensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Terms:\n",
      " [6, 7, 8, 9, 4, 1, 5]\n",
      "Second-order terms:\n",
      " [4.47080778 0.05070979 0.38578491 0.05084816 1.65246964 4.2545994\n",
      " 0.38537311 0.37005553 0.34887244 0.05672495 0.15585817]\n"
     ]
    }
   ],
   "source": [
    "# additional linear terms and second-order terms selected by the algorithm\n",
    "print(\"Linear Terms:\\n\", propensity['lin']) # 4, 1, and 5\n",
    "print(\"Second-order terms:\\n\", propensity['se'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### C. Trim the sample using trim_s to get rid of observations with extreme propensity score values. What is the cut-off that is selected? How many observations are dropped as a result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the sample\n",
    "causal.trim_s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13104228016193686"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the cutoff\n",
    "causal.cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics\n",
      "\n",
      "                       Controls (N_c=256)         Treated (N_t=182)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        4.543        5.501        6.237        7.587        1.694\n",
      "\n",
      "                       Controls (N_c=256)         Treated (N_t=182)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        0.828        0.378        0.841        0.367        0.034\n",
      "             X1        0.109        0.313        0.060        0.239       -0.176\n",
      "             X2       25.074        7.091       25.841        7.208        0.107\n",
      "             X3        0.156        0.364        0.187        0.391        0.081\n",
      "             X4        0.832        0.375        0.714        0.453       -0.283\n",
      "             X5       10.105        1.609       10.297        1.964        0.107\n",
      "             X6        1.675        4.435        1.795        3.876        0.029\n",
      "             X7        0.762        0.427        0.714        0.453       -0.108\n",
      "             X8        1.213        3.052        1.457        3.132        0.079\n",
      "             X9        0.691        0.463        0.604        0.490       -0.182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#observe the data\n",
    "print(causal.summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary above, N_c = 256 means that 4 observations are dropped as a result of the trim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### D. Stratify the sample using stratify_s. How many propensity bins are created? Report the summary statistics for each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify the sample\n",
    "causal.stratify_s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratification Summary\n",
      "\n",
      "              Propensity Score         Sample Size     Ave. Propensity   Outcome\n",
      "   Stratum      Min.      Max.  Controls   Treated  Controls   Treated  Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "         1     0.131     0.379       153        67     0.327     0.332     0.788\n",
      "         2     0.380     0.483        69        63     0.435     0.443     1.587\n",
      "         3     0.487     0.852        34        52     0.596     0.619     3.044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stratification summary will give the number of bins created\n",
    "print(causal.strata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above summary indicates that 3 bins are created by the starify_s method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### E. Estimate the average treatment effect using OLS, blocking, and matching. For matching, set the number of matches to 2 and adjust for bias. How much do the estimates differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: OLS\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.467      0.638      2.299      0.022      0.216      2.718\n",
      "           ATC      1.385      0.652      2.123      0.034      0.106      2.663\n",
      "           ATT      1.583      0.651      2.432      0.015      0.307      2.858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimate average treatment by OLS \n",
    "causal.est_via_ols()\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: OLS\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.467      0.638      2.299      0.022      0.216      2.718\n",
      "           ATC      1.385      0.652      2.123      0.034      0.106      2.663\n",
      "           ATT      1.583      0.651      2.432      0.015      0.307      2.858\n",
      "\n",
      "Treatment Effect Estimates: Blocking\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.542      0.641      2.406      0.016      0.286      2.798\n",
      "           ATC      1.402      0.654      2.145      0.032      0.121      2.683\n",
      "           ATT      1.739      0.663      2.623      0.009      0.440      3.039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimate average treatment by blocking \n",
    "causal.est_via_blocking()\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: OLS\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.467      0.638      2.299      0.022      0.216      2.718\n",
      "           ATC      1.385      0.652      2.123      0.034      0.106      2.663\n",
      "           ATT      1.583      0.651      2.432      0.015      0.307      2.858\n",
      "\n",
      "Treatment Effect Estimates: Blocking\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.542      0.641      2.406      0.016      0.286      2.798\n",
      "           ATC      1.402      0.654      2.145      0.032      0.121      2.683\n",
      "           ATT      1.739      0.663      2.623      0.009      0.440      3.039\n",
      "\n",
      "Treatment Effect Estimates: Matching\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.400      0.888      1.576      0.115     -0.341      3.140\n",
      "           ATC      1.316      0.971      1.356      0.175     -0.587      3.219\n",
      "           ATT      1.517      0.935      1.623      0.105     -0.315      3.350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimate average treatment by matching \n",
    "causal.est_via_matching(matches=2, bias_adj=True)\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates by matching are the least followed by estimates by OLS then Blocking produces the largest estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A. From the module sklearn.datasets, load the training data set using the method fetch_20newsgroups. This dataset comprises around 18000 newsgroups posts on 20 topics. Print out a couple sample posts and list out all the topic names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"From: lerxst@wam.umd.edu (where's my thing)\\n\"\n",
      " 'Subject: WHAT car is this!?\\n'\n",
      " 'Nntp-Posting-Host: rac3.wam.umd.edu\\n'\n",
      " 'Organization: University of Maryland, College Park\\n'\n",
      " 'Lines: 15\\n'\n",
      " '\\n'\n",
      " ' I was wondering if anyone out there could enlighten me on this car I saw\\n'\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/\\n'\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition,\\n'\n",
      " 'the front bumper was separate from the rest of the body. This is \\n'\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years\\n'\n",
      " 'of production, where this car is made, history, or whatever info you\\n'\n",
      " 'have on this funky looking car, please e-mail.\\n'\n",
      " '\\n'\n",
      " 'Thanks,\\n'\n",
      " '- IL\\n'\n",
      " '   ---- brought to you by your neighborhood Lerxst ----\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n')\n",
      "\n",
      "('From: guykuo@carson.u.washington.edu (Guy Kuo)\\n'\n",
      " 'Subject: SI Clock Poll - Final Call\\n'\n",
      " 'Summary: Final call for SI clock reports\\n'\n",
      " 'Keywords: SI,acceleration,clock,upgrade\\n'\n",
      " 'Article-I.D.: shelley.1qvfo9INNc3s\\n'\n",
      " 'Organization: University of Washington\\n'\n",
      " 'Lines: 11\\n'\n",
      " 'NNTP-Posting-Host: carson.u.washington.edu\\n'\n",
      " '\\n'\n",
      " 'A fair number of brave souls who upgraded their SI clock oscillator have\\n'\n",
      " 'shared their experiences for this poll. Please send a brief message '\n",
      " 'detailing\\n'\n",
      " 'your experiences with the procedure. Top speed attained, CPU rated speed,\\n'\n",
      " 'add on cards and adapters, heat sinks, hour of usage per day, floppy disk\\n'\n",
      " 'functionality with 800 and 1.4 m floppies are especially requested.\\n'\n",
      " '\\n'\n",
      " 'I will be summarizing in the next two days, so please add to the network\\n'\n",
      " \"knowledge base if you have done the clock upgrade and haven't answered this\\n\"\n",
      " 'poll. Thanks.\\n'\n",
      " '\\n'\n",
      " 'Guy Kuo <guykuo@u.washington.edu>\\n')\n",
      "\n",
      "('From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\n'\n",
      " 'Subject: PB questions...\\n'\n",
      " 'Organization: Purdue University Engineering Computer Network\\n'\n",
      " 'Distribution: usa\\n'\n",
      " 'Lines: 36\\n'\n",
      " '\\n'\n",
      " 'well folks, my mac plus finally gave up the ghost this weekend after\\n'\n",
      " \"starting life as a 512k way back in 1985.  sooo, i'm in the market for a\\n\"\n",
      " 'new machine a bit sooner than i intended to be...\\n'\n",
      " '\\n'\n",
      " \"i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\\n\"\n",
      " 'of questions that (hopefully) somebody can answer:\\n'\n",
      " '\\n'\n",
      " '* does anybody know any dirt on when the next round of powerbook\\n'\n",
      " \"introductions are expected?  i'd heard the 185c was supposed to make an\\n\"\n",
      " 'appearence \"this summer\" but haven\\'t heard anymore on it - and since i\\n'\n",
      " \"don't have access to macleak, i was wondering if anybody out there had\\n\"\n",
      " 'more info...\\n'\n",
      " '\\n'\n",
      " '* has anybody heard rumors about price drops to the powerbook line like the\\n'\n",
      " \"ones the duo's just went through recently?\\n\"\n",
      " '\\n'\n",
      " \"* what's the impression of the display on the 180?  i could probably swing\\n\"\n",
      " \"a 180 if i got the 80Mb disk rather than the 120, but i don't really have\\n\"\n",
      " 'a feel for how much \"better\" the display is (yea, it looks great in the\\n'\n",
      " 'store, but is that all \"wow\" or is it really that good?).  could i solicit\\n'\n",
      " 'some opinions of people who use the 160 and 180 day-to-day on if its worth\\n'\n",
      " 'taking the disk size and money hit to get the active display?  (i realize\\n'\n",
      " \"this is a real subjective question, but i've only played around with the\\n\"\n",
      " 'machines in a computer store breifly and figured the opinions of somebody\\n'\n",
      " 'who actually uses the machine daily might prove helpful).\\n'\n",
      " '\\n'\n",
      " '* how well does hellcats perform?  ;)\\n'\n",
      " '\\n'\n",
      " \"thanks a bunch in advance for any info - if you could email, i'll post a\\n\"\n",
      " 'summary (news reading time is at a premium with finals just around the\\n'\n",
      " 'corner... :( )\\n'\n",
      " '--\\n'\n",
      " 'Tom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical '\n",
      " 'Engineering\\n'\n",
      " '---------------------------------------------------------------------------\\n'\n",
      " '\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\n'\n",
      " 'Nietzsche\\n')\n",
      "\n",
      "('From: jgreen@amber (Joe Green)\\n'\n",
      " 'Subject: Re: Weitek P9000 ?\\n'\n",
      " 'Organization: Harris Computer Systems Division\\n'\n",
      " 'Lines: 14\\n'\n",
      " 'Distribution: world\\n'\n",
      " 'NNTP-Posting-Host: amber.ssd.csd.harris.com\\n'\n",
      " 'X-Newsreader: TIN [version 1.1 PL9]\\n'\n",
      " '\\n'\n",
      " 'Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n'\n",
      " '> abraxis@iastate.edu writes in article '\n",
      " '<abraxis.734340159@class1.iastate.edu>:\\n'\n",
      " '> > Anyone know about the Weitek P9000 graphics chip?\\n'\n",
      " \"> As far as the low-level stuff goes, it looks pretty nice.  It's got this\\n\"\n",
      " '> quadrilateral fill command that requires just the four points.\\n'\n",
      " '\\n'\n",
      " \"Do you have Weitek's address/phone number?  I'd like to get some \"\n",
      " 'information\\n'\n",
      " 'about this chip.\\n'\n",
      " '\\n'\n",
      " '--\\n'\n",
      " 'Joe Green\\t\\t\\t\\tHarris Corporation\\n'\n",
      " 'jgreen@csd.harris.com\\t\\t\\tComputer Systems Division\\n'\n",
      " '\"The only thing that really scares me is a person with no sense of humor.\"\\n'\n",
      " '\\t\\t\\t\\t\\t\\t-- Jonathan Winters\\n')\n",
      "\n",
      "('From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\n'\n",
      " 'Subject: Re: Shuttle Launch Question\\n'\n",
      " 'Organization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\n'\n",
      " 'Distribution: sci\\n'\n",
      " 'Lines: 23\\n'\n",
      " '\\n'\n",
      " 'From article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A '\n",
      " 'Baker):\\n'\n",
      " '>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) '\n",
      " 'writes...\\n'\n",
      " '>>>\"Clear caution & warning memory.  Verify no unexpected\\n'\n",
      " '>>>errors. ...\".  I am wondering what an \"expected error\" might\\n'\n",
      " '>>>be.  Sorry if this is a really dumb question, but\\n'\n",
      " '> \\n'\n",
      " '> Parity errors in memory or previously known conditions that were '\n",
      " 'waivered.\\n'\n",
      " '>    \"Yes that is an error, but we already knew about it\"\\n'\n",
      " \"> I'd be curious as to what the real meaning of the quote is.\\n\"\n",
      " '> \\n'\n",
      " '> tom\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"My understanding is that the 'expected errors' are basically\\n\"\n",
      " 'known bugs in the warning system software - things are checked\\n'\n",
      " \"that don't have the right values in yet because they aren't\\n\"\n",
      " 'set till after launch, and suchlike. Rather than fix the code\\n'\n",
      " 'and possibly introduce new bugs, they just tell the crew\\n'\n",
      " \"'ok, if you see a warning no. 213 before liftoff, ignore it'.\\n\"\n",
      " '\\n'\n",
      " ' - Jonathan\\n'\n",
      " '\\n'\n",
      " '\\n')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a couple (say 5) sample posts\n",
    "for post in data[:5]:\n",
    "    pprint(post)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# list out all topic names\n",
    "pprint(list(dataset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### B. Convert the posts (blobs of texts) into bag-of-word vectors. What is the dimensionality of these vectors? That is, what is the number of words that have appeared in this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for creting a bag-of-words vector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the text to vector transform object\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 36254)\t0.14365581894428547\n",
      "  (0, 12514)\t0.10213076203339368\n",
      "  (0, 27452)\t0.10684301809548442\n",
      "  (0, 49936)\t0.0601729255075507\n",
      "  (0, 32912)\t0.06540016969537528\n",
      "  (0, 32219)\t0.07543341899600832\n",
      "  (0, 23695)\t0.1701883013418222\n",
      "  (0, 28064)\t0.07985173645101952\n",
      "  (0, 26362)\t0.09109018303700456\n",
      "  (0, 40860)\t0.11821340713192517\n",
      "  (0, 55580)\t0.06619015166091179\n",
      "  (0, 47259)\t0.11840797817328158\n",
      "  (0, 20932)\t0.10607029880882898\n",
      "  (0, 34893)\t0.096567268121596\n",
      "  (0, 49706)\t0.19316713537009014\n",
      "  (0, 30669)\t0.04674013229588189\n",
      "  (0, 11933)\t0.0951701162387341\n",
      "  (0, 43333)\t0.08921734695029614\n",
      "  (0, 45542)\t0.10776256116529917\n",
      "  (0, 12728)\t0.145005721890891\n",
      "  (0, 7903)\t0.10410634610867979\n",
      "  (0, 46669)\t0.08217269860173604\n",
      "  (0, 42366)\t0.061870276832261296\n",
      "  (0, 19464)\t0.12669421106937861\n",
      "  (0, 12395)\t0.18248258600997347\n",
      "  :\t:\n",
      "  (4, 37297)\t0.10470101009575183\n",
      "  (4, 9941)\t0.12926268019328424\n",
      "  (4, 46716)\t0.12632823936080279\n",
      "  (4, 31282)\t0.1764594089841088\n",
      "  (4, 46124)\t0.09269312093643926\n",
      "  (4, 33620)\t0.12072229015841428\n",
      "  (4, 25755)\t0.08394905709969103\n",
      "  (4, 14088)\t0.11657256546626804\n",
      "  (4, 25888)\t0.06993095113747155\n",
      "  (4, 29252)\t0.1486404598718769\n",
      "  (4, 29628)\t0.18538624187287853\n",
      "  (4, 15313)\t0.05985326574720713\n",
      "  (4, 54655)\t0.08923774730252867\n",
      "  (4, 50469)\t0.15377215825175872\n",
      "  (4, 41731)\t0.10139809473262283\n",
      "  (4, 42346)\t0.056276528069909605\n",
      "  (4, 29880)\t0.035012259199459335\n",
      "  (4, 19419)\t0.0356850345326232\n",
      "  (4, 21783)\t0.16193300008641678\n",
      "  (4, 36398)\t0.04239455266466661\n",
      "  (4, 52380)\t0.04803759873641572\n",
      "  (4, 19143)\t0.03955515332763504\n",
      "  (4, 9698)\t0.05676754849852974\n",
      "  (4, 42366)\t0.04760864928254053\n",
      "  (4, 54588)\t0.07462852605769851\n"
     ]
    }
   ],
   "source": [
    "# create the bag of word vector\n",
    "vectors = vectorizer.fit_transform(dataset.data)\n",
    "print(vectors[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### C. Use your favorite dimensionality reduction technique to compress these vectors into ones of K = 30 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated SVD for dimensionality reduction\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dimensionality reduction SVD object\n",
    "K = 30\n",
    "svd = TruncatedSVD(n_components=K, algorithm='randomized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the dimensionality reduction with transform\n",
    "svd_vectors = svd.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### D. Use your favorite supervised learning model to train a model that tries to predict the topic of a post from the vectorized representation of the post you obtained in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DecisionTreeClassifeir for the model\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "clf.fit(svd_vectors, dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a prediction\n",
    "clf.predict(svd_vectors[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### E. Use the test data to tune your model. Make sure to include K as a hyperparameter as well. Use accuracy_score from sklearn.metrics as your evaluation metric. What is the highest accuracy you are able to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model with K max features hyperparameter\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=3, max_features=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, max_features=30, random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(svd_vectors, dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the test data\n",
    "test_dataset = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bag of words vector for the test data\n",
    "vectors = vectorizer.fit_transform(test_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform dimensionality reduction\n",
    "svd_vectors = svd.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score metric\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_dataset.target  # the true target values in the dataset\n",
    "y_pred = clf.predict(svd_vectors)  # the predicted target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the accuracy score\n",
    "score = accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.17\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score %0.2f\" %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
